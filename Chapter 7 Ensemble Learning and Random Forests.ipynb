{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning and Random Forests\n",
    "\n",
    "** Use a group of model to make a classification or Regression**\n",
    "\n",
    "## Voting Classifiers\n",
    "\n",
    "* A simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifer.\n",
    "\n",
    "* Ensemble methods work best when the predictors are as independent from one another as possible.\n",
    "\n",
    "* One way to get diverse classifiers is to train them using very different algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf),('rf', rnd_clf),('svc', svm_clf)],\n",
    "                             voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.888\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "#Look at each classifier's accuracy on the test set:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If all classifiers are able to estimate class probabilites, we can tell sklearn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting.\n",
    "\n",
    "* Soft voting often achieves higher performance than hard voiting.\n",
    "\n",
    "## Bagging and Pasting\n",
    "\n",
    "* Another way to get a diverse set of classifiers is to use the same training algorithm for every predictor, but to train on different random subsets of the training set. \n",
    "\n",
    "* When sampling is performed with replacement, This method is called bagging(short for bootstrap aggregating)\n",
    "\n",
    "* When sampling is performed without replacement, it is called pasting.\n",
    "\n",
    "* Generally, the net result is that the ensemble has a similar bias but a lower variance than single predictor trained on the original training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                           max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "\n",
    "* With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. Only about 63% of the training instances are sampled on average for each predictor. The remaining 37% of the training instances that are not sampled are called out-of-bag(oob) instances. \n",
    "\n",
    "* Since a predictor never sees the oob instances during training, it can be evaluated on these instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89600000000000002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                           bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90400000000000003"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32571429,  0.67428571],\n",
       "       [ 0.31638418,  0.68361582],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.08333333,  0.91666667],\n",
       "       [ 0.31976744,  0.68023256],\n",
       "       [ 0.01492537,  0.98507463],\n",
       "       [ 0.98421053,  0.01578947],\n",
       "       [ 0.98387097,  0.01612903],\n",
       "       [ 0.75129534,  0.24870466],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.79166667,  0.20833333],\n",
       "       [ 0.87951807,  0.12048193],\n",
       "       [ 0.98019802,  0.01980198],\n",
       "       [ 0.05820106,  0.94179894],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.96842105,  0.03157895],\n",
       "       [ 0.95652174,  0.04347826],\n",
       "       [ 0.99378882,  0.00621118],\n",
       "       [ 0.02777778,  0.97222222],\n",
       "       [ 0.35802469,  0.64197531],\n",
       "       [ 0.90865385,  0.09134615],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.95808383,  0.04191617],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.99470899,  0.00529101],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.65240642,  0.34759358],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.16666667,  0.83333333],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.32786885,  0.67213115],\n",
       "       [ 0.0052356 ,  0.9947644 ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.24      ,  0.76      ],\n",
       "       [ 0.3559322 ,  0.6440678 ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00549451,  0.99450549],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.02352941,  0.97647059],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01621622,  0.98378378],\n",
       "       [ 0.97619048,  0.02380952],\n",
       "       [ 0.89010989,  0.10989011],\n",
       "       [ 0.955     ,  0.045     ],\n",
       "       [ 0.96969697,  0.03030303],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.02747253,  0.97252747],\n",
       "       [ 0.99418605,  0.00581395],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.00591716,  0.99408284],\n",
       "       [ 0.99468085,  0.00531915],\n",
       "       [ 0.80434783,  0.19565217],\n",
       "       [ 0.38554217,  0.61445783],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.72486772,  0.27513228],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.89772727,  0.10227273],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.6091954 ,  0.3908046 ],\n",
       "       [ 0.08695652,  0.91304348],\n",
       "       [ 0.61271676,  0.38728324],\n",
       "       [ 0.88265306,  0.11734694],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.20224719,  0.79775281],\n",
       "       [ 0.875     ,  0.125     ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98907104,  0.01092896],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.05172414,  0.94827586],\n",
       "       [ 0.01612903,  0.98387097],\n",
       "       [ 0.29166667,  0.70833333],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.87356322,  0.12643678],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.2642487 ,  0.7357513 ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.00540541,  0.99459459],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.95698925,  0.04301075],\n",
       "       [ 0.79503106,  0.20496894],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.28282828,  0.71717172],\n",
       "       [ 0.640625  ,  0.359375  ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.04918033,  0.95081967],\n",
       "       [ 0.5       ,  0.5       ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01111111,  0.98888889],\n",
       "       [ 0.98314607,  0.01685393],\n",
       "       [ 0.21105528,  0.78894472],\n",
       "       [ 0.49246231,  0.50753769],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01578947,  0.98421053],\n",
       "       [ 0.99441341,  0.00558659],\n",
       "       [ 0.29239766,  0.70760234],\n",
       "       [ 0.92746114,  0.07253886],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.77835052,  0.22164948],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01754386,  0.98245614],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.99494949,  0.00505051],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.92397661,  0.07602339],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01075269,  0.98924731],\n",
       "       [ 0.20571429,  0.79428571],\n",
       "       [ 0.94607843,  0.05392157],\n",
       "       [ 0.31891892,  0.68108108],\n",
       "       [ 0.98275862,  0.01724138],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.75129534,  0.24870466],\n",
       "       [ 0.37951807,  0.62048193],\n",
       "       [ 0.47222222,  0.52777778],\n",
       "       [ 0.82564103,  0.17435897],\n",
       "       [ 0.91489362,  0.08510638],\n",
       "       [ 0.04278075,  0.95721925],\n",
       "       [ 0.78531073,  0.21468927],\n",
       "       [ 0.0104712 ,  0.9895288 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.02604167,  0.97395833],\n",
       "       [ 0.98360656,  0.01639344],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00595238,  0.99404762],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.01104972,  0.98895028],\n",
       "       [ 0.00526316,  0.99473684],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.96835443,  0.03164557],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.99494949,  0.00505051],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.31707317,  0.68292683],\n",
       "       [ 0.27322404,  0.72677596],\n",
       "       [ 0.01086957,  0.98913043],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.34482759,  0.65517241],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.99444444,  0.00555556],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.62087912,  0.37912088],\n",
       "       [ 0.92      ,  0.08      ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98324022,  0.01675978],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.07317073,  0.92682927],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.02645503,  0.97354497],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.01515152,  0.98484848],\n",
       "       [ 0.99473684,  0.00526316],\n",
       "       [ 0.95054945,  0.04945055],\n",
       "       [ 0.74725275,  0.25274725],\n",
       "       [ 0.53846154,  0.46153846],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.10471204,  0.89528796],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.96236559,  0.03763441],\n",
       "       [ 0.96875   ,  0.03125   ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00558659,  0.99441341],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.43258427,  0.56741573],\n",
       "       [ 0.85714286,  0.14285714],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.02094241,  0.97905759],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.94475138,  0.05524862],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.35555556,  0.64444444],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98850575,  0.01149425],\n",
       "       [ 0.81395349,  0.18604651],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.05882353,  0.94117647],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01675978,  0.98324022],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.07909605,  0.92090395],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.8045977 ,  0.1954023 ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.9122807 ,  0.0877193 ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.20873786,  0.79126214],\n",
       "       [ 0.22613065,  0.77386935],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.24277457,  0.75722543],\n",
       "       [ 0.94148936,  0.05851064],\n",
       "       [ 0.00549451,  0.99450549],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.99435028,  0.00564972],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.43888889,  0.56111111],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00632911,  0.99367089],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.09782609,  0.90217391],\n",
       "       [ 0.11398964,  0.88601036],\n",
       "       [ 0.98387097,  0.01612903],\n",
       "       [ 0.00568182,  0.99431818],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.39204545,  0.60795455],\n",
       "       [ 0.14917127,  0.85082873],\n",
       "       [ 0.47058824,  0.52941176],\n",
       "       [ 0.59693878,  0.40306122],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.67052023,  0.32947977],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.23762376,  0.76237624],\n",
       "       [ 0.835     ,  0.165     ],\n",
       "       [ 0.07246377,  0.92753623],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.81034483,  0.18965517],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.01104972,  0.98895028],\n",
       "       [ 0.10994764,  0.89005236],\n",
       "       [ 0.00531915,  0.99468085],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.99418605,  0.00581395],\n",
       "       [ 0.90862944,  0.09137056],\n",
       "       [ 0.15517241,  0.84482759],\n",
       "       [ 0.94857143,  0.05142857],\n",
       "       [ 0.00581395,  0.99418605],\n",
       "       [ 0.55497382,  0.44502618],\n",
       "       [ 0.07960199,  0.92039801],\n",
       "       [ 0.99435028,  0.00564972],\n",
       "       [ 0.79032258,  0.20967742],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.93491124,  0.06508876],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.2513089 ,  0.7486911 ],\n",
       "       [ 0.99408284,  0.00591716],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.87234043,  0.12765957],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.7195122 ,  0.2804878 ],\n",
       "       [ 0.9281768 ,  0.0718232 ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.60989011,  0.39010989],\n",
       "       [ 0.42285714,  0.57714286],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.9125    ,  0.0875    ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.89502762,  0.10497238],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.74585635,  0.25414365],\n",
       "       [ 0.11111111,  0.88888889],\n",
       "       [ 0.44318182,  0.55681818],\n",
       "       [ 0.18831169,  0.81168831],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.87894737,  0.12105263],\n",
       "       [ 0.83333333,  0.16666667],\n",
       "       [ 0.00526316,  0.99473684],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00534759,  0.99465241],\n",
       "       [ 0.02590674,  0.97409326],\n",
       "       [ 0.94708995,  0.05291005],\n",
       "       [ 0.93717277,  0.06282723],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.53723404,  0.46276596],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.99441341,  0.00558659],\n",
       "       [ 0.00534759,  0.99465241],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.96273292,  0.03726708],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.05945946,  0.94054054],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.15135135,  0.84864865],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.41884817,  0.58115183],\n",
       "       [ 0.05464481,  0.94535519],\n",
       "       [ 0.24242424,  0.75757576],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.98947368,  0.01052632],\n",
       "       [ 0.23076923,  0.76923077],\n",
       "       [ 0.99447514,  0.00552486],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.96470588,  0.03529412],\n",
       "       [ 0.27894737,  0.72105263],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.98404255,  0.01595745],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.01796407,  0.98203593],\n",
       "       [ 0.97109827,  0.02890173],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01052632,  0.98947368],\n",
       "       [ 0.59340659,  0.40659341]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "\n",
    "* Sampling both training instances and features is called the Random Patches method.\n",
    "* Keeping all training instances but sampling features is called the Random Subspaces method.\n",
    "* Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "* Random Forests is an ensemble of Decision Trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92000000000000004"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_rf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Trees\n",
    "\n",
    "* When we are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting.\n",
    "\n",
    "* A forest of sunch extremely random trees is simply called an Extremely Randomized Trees ensemble (or Extra-Trees for short). this trades more bias for a lower variance.\n",
    "\n",
    "## Feature Importance\n",
    "\n",
    "* In a single Decision Tree, important features are likely to appear closer to the root of the tree, while unimportant features will often appear closer to the leaves. \n",
    "\n",
    "* It is therefore possible to get an estimate of a feature's importance by computing the average depth at which it appears across all trees in the forest.\n",
    "\n",
    "* we can access the result using the feature_importances_ variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.0956045546963\n",
      "sepal width (cm) 0.0221267719485\n",
      "petal length (cm) 0.418076917646\n",
      "petal width (cm) 0.464191755709\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting ( hypothesis boosting)\n",
    "\n",
    "* The general idea of most boosting methods is to train predictors sequentially, each tring to correct its predecessor. \n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "* One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases.\n",
    "\n",
    "* There is one important drawback to this sequential learning technique: it cannot be parallelized, since each predictor can only be trained after the previous predictor has been trained and evaluated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,algorithm=\"SAMME.R\",\n",
    "                            learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89600000000000002"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf_pred = ada_clf.predict(X_test)\n",
    "accuracy_score(ada_clf_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "* Gradient Boosting works by sequentially adding predictors to an ensemble. This method tries to fit the new predictor to the residual errors made by the previous predicor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:,0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new = np.array([[0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75026781])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=3, presort='auto',\n",
       "             random_state=None, subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The easy way\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbrt_pred = gbrt.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75026781])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking(short for stacked generalization)\n",
    "\n",
    "* This ensemble method is based on a simple idea: instead of using trivial functions(such as hard voting) to aggregate the predicitons of all predictors in an ensemble, why don't we train a model to perform this aggregation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
