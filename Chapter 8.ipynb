{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "* Many Machine Learning problems involve thousands or even millions of features for each training instance. Not only does this make training extremely slow, it can also make it much harder to find a good solution, as we will see. This problem is often referred to as the curse of dimensionality.\n",
    "\n",
    "* Reducing dimensionality does lose some imformation, it may also make our system perform slightly worse. we should first try to train our system with the original data before considering using dimensionality reduction if training is too slow.\n",
    "\n",
    "* Reducing the dimensionality of training data may filter out some noise and unnecessary details and thus result in higher performance(but in general it won't)\n",
    "\n",
    "## The Curse of Dimensionality\n",
    "\n",
    "* High-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other.\n",
    "\n",
    "* A new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions.\n",
    "\n",
    "* The more dimensions the training set has, the greater the risk of overfitting it.\n",
    "\n",
    "## Main Approaches for Dimensionality Reduction\n",
    "\n",
    "### Projection\n",
    "\n",
    "* In most real-world problems, training instances are not spread out uniformly across all dimensions. all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space.\n",
    "\n",
    "* Projection is not always the best approach to dimensionality reduction. In many case the subspace may twist and turn.\n",
    "\n",
    "### Manifold Learning\n",
    "\n",
    "* a d-D manifold is a d-D shape that can be bent and twisted in a higher-dimensional space.\n",
    "\n",
    "* Manifold Learning relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold.\n",
    "\n",
    "* The manifold assumption is often accompanied by another implicit assumption: that the task at hand(e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold. (this assumption does not always hold)\n",
    "\n",
    "* If we reduce the dimensionality of our training set before training a model, it will definitely speed up training, but it may not always lead to a better or simpler solution.\n",
    "\n",
    "## PCA (Principal Component Analysis)\n",
    "\n",
    "* The PCA identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
    "\n",
    "### Preserving the Variance\n",
    "\n",
    "* Select the axis that preserves the maximum amount of variance will most likely lose less information than the other projections.\n",
    "\n",
    "* Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis.\n",
    "\n",
    "### Principal Components\n",
    "\n",
    "* The unit vector that defines the ith axis is called the ith principal component(PC).\n",
    "\n",
    "* We can use Singular Value Decomposition to find the principal components of a training set.\n",
    "\n",
    "### Projecting Down to d Dimensions\n",
    "\n",
    "* Once we have identified all the principal components, we can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components.\n",
    "\n",
    "### Using sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# build 3D dataset\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance Ratio\n",
    "\n",
    "* Explained Variance Ratio indicated the proportion of the dataset's variance that lies along the axis of each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84248607  0.14631839]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions\n",
    "\n",
    "* It is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance(e.g., 95%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the same but better way\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.26203346,  0.42067648],\n",
       "       [-0.08001485, -0.35272239],\n",
       "       [ 1.17545763,  0.36085729],\n",
       "       [ 0.89305601, -0.30862856],\n",
       "       [ 0.73016287, -0.25404049],\n",
       "       [-1.10436914,  0.20204953],\n",
       "       [ 1.27265808,  0.46781247],\n",
       "       [-0.44933007,  0.67736663],\n",
       "       [-1.09356195, -0.04467792],\n",
       "       [-0.66177325, -0.28651264],\n",
       "       [ 1.04466138, -0.11244353],\n",
       "       [-1.05932502,  0.31189109],\n",
       "       [ 1.13761426,  0.14576655],\n",
       "       [ 1.16044117,  0.36481599],\n",
       "       [-1.00167625,  0.39422008],\n",
       "       [ 0.2750406 , -0.34391089],\n",
       "       [-0.45624787,  0.69707573],\n",
       "       [-0.79706574, -0.26870969],\n",
       "       [-0.66924929,  0.65520024],\n",
       "       [ 1.30679728,  0.37671343],\n",
       "       [-0.6626586 , -0.32706423],\n",
       "       [ 1.25387588,  0.56043928],\n",
       "       [ 1.04046987, -0.08727672],\n",
       "       [ 1.26047729,  0.1571074 ],\n",
       "       [-1.09786649,  0.38643428],\n",
       "       [-0.7130973 ,  0.64941523],\n",
       "       [ 0.17786909, -0.43609071],\n",
       "       [-1.02975735,  0.33747452],\n",
       "       [ 0.94552283, -0.22833268],\n",
       "       [-0.80994916, -0.33810729],\n",
       "       [-0.20189175, -0.3514758 ],\n",
       "       [ 1.34219411,  0.42415687],\n",
       "       [-0.13599883, -0.37258632],\n",
       "       [-0.8206931 ,  0.55120835],\n",
       "       [-0.90818634,  0.31869127],\n",
       "       [-0.06703671, -0.42486148],\n",
       "       [-0.13936893, -0.41906961],\n",
       "       [ 0.37356775, -0.27320849],\n",
       "       [-0.7312441 , -0.23441131],\n",
       "       [ 0.5230355 , -0.46621776],\n",
       "       [-0.86146183, -0.30212526],\n",
       "       [ 0.33203239, -0.47352674],\n",
       "       [ 0.99467436, -0.18342807],\n",
       "       [-1.04520043,  0.32697207],\n",
       "       [-0.87477048, -0.18062856],\n",
       "       [-0.30457923, -0.43904343],\n",
       "       [ 0.63685997, -0.32851826],\n",
       "       [-1.1287259 ,  0.11627335],\n",
       "       [-0.03836205, -0.49036349],\n",
       "       [ 0.41386843, -0.31734423],\n",
       "       [ 1.32417938,  0.1944472 ],\n",
       "       [-0.92968677, -0.18429606],\n",
       "       [ 0.40274964, -0.34154025],\n",
       "       [-1.11480941,  0.24138847],\n",
       "       [-0.31915065, -0.27787663],\n",
       "       [-1.02666316,  0.34676546],\n",
       "       [ 1.24145806,  0.35049349],\n",
       "       [-0.66770361, -0.32262317],\n",
       "       [ 1.16397896, -0.03648137],\n",
       "       [-0.68326064, -0.22756871]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another option is to plot the explained variance as a function of the number of dimensions. There will usually be an elbow in the curve, where the explained variance stops growing fast. we can think of this as the intrinsic dimensionality of the dataset. \n",
    "\n",
    "### PCA for Compression\n",
    "\n",
    "* Applying PCA to the MNIST dataset while preserving 95% of its variance. we should find that each instance will have just over 150 features, instead of the original 784 features. So while most of the variance is preserved, the dataset is now less than 20% of its orginal size.\n",
    "\n",
    "* It is also possible to decompress the reduced dataset by applying the inverse transformation of the PCA projection. Of course this won't give we back the original data, since the projection lost a bit of information.\n",
    "\n",
    "* The mean squared distance between the original data and the reconstructed data is called the reconstrcution error.\n",
    "\n",
    "### Incremental PCA\n",
    "\n",
    "* One problem with the preceding implementation of PCA is that it requires the whole training set to fit in memory in order for the SVD algorithm to run.\n",
    "\n",
    "* Incremental PCA(IPCA) algorithms have been developed: we can split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time. \n",
    "\n",
    "### Randomized PCA\n",
    "\n",
    "* Sklearn offers another option to perform PCA, called Randomized PCA. This is a stochastic algorithm that quickly finds an approximation of the first d principal components. Its computaional complexity is O(m×d^2) + O(d^3), instead of O(m×n^2) + O(n^3), so it is dramatically faster than the previous algorithms when d is much smaller than n.\n",
    "\n",
    "### Kernal PCA\n",
    "\n",
    "* use kernal trick in PCA method.\n",
    "\n",
    "## LLE (Locally Linear Embedding)\n",
    "\n",
    "* LLE is a very powerful nonlinear dimensionality reduction(NLDR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
