{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Nets\n",
    "\n",
    "** Some of the problem about DNN**\n",
    "\n",
    "1. We would be faced with the tricky vanishing gradients problem(or the related exploding gradients problem) that affects deep neural networks and makes lower layer very hard to train.\n",
    "\n",
    "2. With such a large network, training would be extremly slow.\n",
    "\n",
    "3. A model with millions of parameters would severely risk overfitting the training set.\n",
    "\n",
    "## Vanishing/Exploding Gradients Problems\n",
    "\n",
    "* Gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution.(vanishing gradients problem)\n",
    "\n",
    "* In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. (exploding gradients problem, mostly encountered in recurrent neural networks)\n",
    "\n",
    "* Deep neural networks suffer from unstable gradients; different layers may leran at widely different speeds.\n",
    "\n",
    "### Xavier and He Initialization\n",
    "\n",
    "* the connection weights must be initialized randomly in this way.\n",
    "\n",
    "**Normal distribution with mean 0 and standard deviation Sigma = sqrt(2/(n_inputs + n_outputs)) **\n",
    "\n",
    "** Or a uniform distribution between -r and +r, with r = sqrt(6/(n_inputs + n_outputs)) **\n",
    "\n",
    "** This initialization strategy is often called Xavier initialzation, or Glorot initialization.**\n",
    "\n",
    "* There are different initialization strategy between different activation function. The above strategy is for sigmoid activation function.\n",
    "\n",
    "### Nonsaturating Activation Functions\n",
    "\n",
    "* the Relu activation function does not saturate for positive values and it is quite fast to compute.\n",
    "\n",
    "* Unfortunately, the ReLU activation function suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. \n",
    "\n",
    "* To solve this problem, we may want to use a variant of the ReLU function, such as the leaky ReLU. \n",
    "\n",
    "* In general ELU > leaky ReLU > ReLU > tanh > logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
