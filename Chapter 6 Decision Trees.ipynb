{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "* It can perform both classification and regression tasks, and multiouput tasks.\n",
    "\n",
    "* It is the fundamental components of Random Forests.\n",
    "\n",
    "## Training and Visualizing a Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] #petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree_clf, out_file = \"iris_tree.dot\",\n",
    "               feature_names = iris.feature_names[2:],\n",
    "               class_names=iris.target_names,\n",
    "               rounded=True,\n",
    "               filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predicitons\n",
    "\n",
    "* One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don't require feature scaling or centering at all.\n",
    "\n",
    "* A node's value attribute tells us how many training instances of each class this node applies to.\n",
    "\n",
    "* A node's gini attribute measures its impurity: a node is \"pure\"(gini=0) if all training instances it applies to belong to the same calss.\n",
    "\n",
    "* Sklearn uses the CART algorithm, which produces only binary trees: nonleaf nodes always have two children.\n",
    "\n",
    "* Decision Trees provide nice and simple classification rules that can even be applied manually if need be.\n",
    "\n",
    "## Estimating Class Probabilities\n",
    "\n",
    "* A Decision Tree can also estimate the probability that an instance belongs to a particular class k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.90740741,  0.09259259]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm\n",
    "\n",
    "* CART: Classification And Regression Tree\n",
    "\n",
    "* the CART algorithm is a greedy algorithm. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution.\n",
    "\n",
    "* Finding the optimal tree is known to be an NP-Complete problem.\n",
    "\n",
    "## Computaional Complexity\n",
    "\n",
    "* the prediction complexity is just O(log_2(m)), independent of the number of features.\n",
    "\n",
    "* the training complexity is O(n√ómlog(m)).\n",
    "\n",
    "## Gini Impurity or Entropy?\n",
    "\n",
    "* by setting hte criterion hyperparameter to \"entropy\".\n",
    "\n",
    "* entropy is zero when all messages are identical.\n",
    "\n",
    "* In Machine Learning, a set's entropy is zero when it contains instances of only one class.\n",
    "\n",
    "* Gini and entropy does not make a big difference. Gini impurity is slightly faster to compute.\n",
    "\n",
    "### Regularization Hyperparameters\n",
    "\n",
    "* Decision Trees make very few assumptions about the training data. \n",
    "\n",
    "* If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely, and most likely overfitting it.\n",
    "\n",
    "* Such a model is ofen called a nonparametric model.\n",
    "\n",
    "* To avoid overfitting the training data, we need to restrict the Decision Tree's freedom during training.\n",
    "\n",
    "** max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes, max_features)**\n",
    "\n",
    "** Increasing min hyperparameters or reducing max hyperparameters will regularize the model **\n",
    "\n",
    "\n",
    "### Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The CART algorithm works mostly the same way as earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE.\n",
    "\n",
    "### Instability\n",
    "\n",
    "** Limitation of Decision Trees:**\n",
    "\n",
    "1. Decision Trees love orthogonal decision boundaries, which makes them sensitive to training set rotation. One way to limit this problem is to use PCA.\n",
    "\n",
    "2. They are very sensitive to small variations in the training data.\n",
    "\n",
    "3. we can use random forests to solve all these problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
